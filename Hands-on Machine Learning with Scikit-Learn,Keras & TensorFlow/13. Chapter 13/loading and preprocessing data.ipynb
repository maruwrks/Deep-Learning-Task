{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa0khi6bYQYHusa2qJM3xK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maruwrks/Deep-Learning-Task/blob/main/13.Chapter_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Vy1gAMXmTfQy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Prep"
      ],
      "metadata": {
        "id": "NTQiOeZSXBSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
      ],
      "metadata": {
        "id": "O-Lqb2J8T-JT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Normalisasi data"
      ],
      "metadata": {
        "id": "FtgTDIRjXHYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_mean = scaler.mean_\n",
        "X_std = scaler.scale_"
      ],
      "metadata": {
        "id": "LHIrtrekUF8q"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
        "    os.makedirs(housing_dir, exist_ok=True)\n",
        "    path_format = os.path.join(housing_dir, f\"my_{name_prefix}_{{:02d}}.csv\")\n",
        "\n",
        "    filepaths = []\n",
        "    m = len(data)\n",
        "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
        "        part_csv = path_format.format(file_idx)\n",
        "        filepaths.append(part_csv)\n",
        "        with open(part_csv, \"w\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header)\n",
        "                f.write(\"\\n\")\n",
        "            for row_idx in row_indices:\n",
        "                f.write(\",\".join([str(col) for col in data[row_idx]]))\n",
        "                f.write(\"\\n\")\n",
        "    return filepaths"
      ],
      "metadata": {
        "id": "ijm7hGdaUHJS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.c_[X_train, y_train]\n",
        "valid_data = np.c_[X_valid, y_valid]\n",
        "test_data = np.c_[X_test, y_test]\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
        "header = \",\".join(header_cols)"
      ],
      "metadata": {
        "id": "iTaXabMwUMOc"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Save to File"
      ],
      "metadata": {
        "id": "0Hj5lzLiXNdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
        "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
        "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
      ],
      "metadata": {
        "id": "LF1JgYvzUNpH"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pipelining"
      ],
      "metadata": {
        "id": "Prz1Q_EdXXtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_csv_line(line):\n",
        "    defs = [0.] * len(header_cols) # Default untuk setiap kolom adalah float\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = tf.stack(fields[-1:])\n",
        "    return x, y\n",
        "\n",
        "def csv_reader_dataset(filepaths, n_readers=5, n_parse_threads=5,\n",
        "                       n_shuffle=10000, batch_size=32):\n",
        "    # Membuat dataset dari path file\n",
        "    dataset = tf.data.Dataset.list_files(filepaths)\n",
        "    # Membaca dari beberapa file secara paralel\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "        cycle_length=n_readers)\n",
        "    # Parsing baris CSV secara paralel\n",
        "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_threads)\n",
        "    # Shuffle data\n",
        "    dataset = dataset.shuffle(n_shuffle)\n",
        "    # Batch data dan prefetch\n",
        "    return dataset.batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "4e_bM_2AUtDZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Membuat dataset untuk training"
      ],
      "metadata": {
        "id": "zhSiP0jBXhvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = csv_reader_dataset(train_filepaths, batch_size=32)\n",
        "valid_set = csv_reader_dataset(valid_filepaths, batch_size=32)\n",
        "test_set = csv_reader_dataset(test_filepaths, batch_size=32)"
      ],
      "metadata": {
        "id": "EVvHdn9wUucO"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Model"
      ],
      "metadata": {
        "id": "Ofl-H_YtXkhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Standardization(keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mean = tf.constant(X_mean, dtype=tf.float32)\n",
        "        self.std = tf.constant(X_std, dtype=tf.float32)\n",
        "    def call(self, inputs):\n",
        "        return (inputs - self.mean) / self.std"
      ],
      "metadata": {
        "id": "5JilpsAWUyV2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = housing.data.shape[1]\n",
        "model = keras.models.Sequential([\n",
        "    Standardization(input_shape=[n_features]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VrUBZn5Uztw",
        "outputId": "31517f00-588a-4d36-9911-ee9e6021637f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-1205438358>:3: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Model"
      ],
      "metadata": {
        "id": "dwecKjHJXqLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"RootMeanSquaredError\"])\n",
        "history = model.fit(train_set, epochs=5, validation_data=valid_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXmQFuc7U07c",
        "outputId": "28bd33e5-4f2d-4391-d167-d2f1ae8e0c0c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "    360/Unknown \u001b[1m4s\u001b[0m 3ms/step - RootMeanSquaredError: 1.6752 - loss: 2.9022"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - RootMeanSquaredError: 1.6711 - loss: 2.8888 - val_RootMeanSquaredError: 1.1982 - val_loss: 1.4356\n",
            "Epoch 2/5\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - RootMeanSquaredError: 0.8475 - loss: 0.7192 - val_RootMeanSquaredError: 0.8805 - val_loss: 0.7754\n",
            "Epoch 3/5\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.7273 - loss: 0.5295 - val_RootMeanSquaredError: 1.1835 - val_loss: 1.4006\n",
            "Epoch 4/5\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.6943 - loss: 0.4831 - val_RootMeanSquaredError: 0.6804 - val_loss: 0.4630\n",
            "Epoch 5/5\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.6525 - loss: 0.4259 - val_RootMeanSquaredError: 0.8102 - val_loss: 0.6563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluasi"
      ],
      "metadata": {
        "id": "g_NR2jlDXtwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nEvaluasi pada test set:\")\n",
        "mse_test, rmse_test = model.evaluate(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iErJRgAXU26J",
        "outputId": "74d35081-ba7d-4ca7-fe86-8cab8ec3ff23"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluasi pada test set:\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 0.6475 - loss: 0.4203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMembuat prediksi pada data baru:\")\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kLYKofkU50x",
        "outputId": "9c043379-fe0b-4dc1-deea-9dc8f4d37822"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Membuat prediksi pada data baru:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Prediksi:\", y_pred.flatten())\n",
        "print(\"Label Sebenarnya:\", y_test[:3].flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgc5uOaHU63D",
        "outputId": "8bfb57f2-fcae-40cd-d016-28243fd9afb9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediksi: [0.38177377 1.7963195  3.5372946 ]\n",
            "Label Sebenarnya: [0.477   0.458   5.00001]\n"
          ]
        }
      ]
    }
  ]
}
