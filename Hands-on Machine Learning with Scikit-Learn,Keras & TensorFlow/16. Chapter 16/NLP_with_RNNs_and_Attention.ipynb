{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#All"
      ],
      "metadata": {
        "id": "9Ag0VN5pHF0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjJGkWFMEFfv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Prep"
      ],
      "metadata": {
        "id": "BIFWvZeFERDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A5zC_zYELso",
        "outputId": "a7e95957-8d17-4b5b-f7a7-706f9fb9b694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "metadata": {
        "id": "tSbsHrrzET8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
        "train_size = len(encoded) * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "metadata": {
        "id": "NFAkyVbTEVpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 100\n",
        "window_length = n_steps + 1\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
      ],
      "metadata": {
        "id": "9DmsMOpAEXiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ],
      "metadata": {
        "id": "DKtvITFHEZU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "train_set, test_set = datasets[\"train\"], datasets[\"test\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "db29ffedc89a4d74a58ea7e048bb81df",
            "1e02a76a3ded4a709f2fdf85ebcb640c",
            "0577dca59cee40ca811d2d4bc0e29418",
            "3e50d913c2674de5aef182d8adebcd69",
            "c44803f1e6ef4b4f943dd190be7952bb",
            "b8c994ca81a24488b0299a42217d0545",
            "91cf53b52ef64dc8a0d618ccb1993054",
            "6d4d5d520b144b48a5b4beab4ca8765e",
            "2a17015a7aa6411dbab83011f7f76b2e",
            "89d8167ee0954b23a1558a720e4c8458",
            "695371c87417492497cb5269a2c42273",
            "5c518a52805a41db85d669d9a45199a8",
            "da64b827f5b240f5839f4a5e6ac668b2",
            "87e6039ac9f042d6a2338726deb95500",
            "da290046fbe64ea98a0b9c0d5b057d6e",
            "70d03962d591461ea33649e0294bd33f",
            "0575aa7e9a8c4f9f8bf2827058d24812",
            "67129eecb4a144598ad585573932ec52",
            "7c7d857af8944b4299382e2473fed6cb",
            "dd21837429ca47858aa2fe23a9e80859",
            "40c2ad4f906e45e3bd66a272520924f2",
            "a5d670be1b1d41b88005c4d026110b93",
            "2da35449134c4b82b5fee72f04ebd0db",
            "a6f8846ca3e14e529a5356ea8371d63b",
            "64b26af7bff843029396bf81d89abfa7",
            "d67b5ed9f15b4e2593f6396517f37b06",
            "d20cf1eca9cc4dc2ad74f177706daa37",
            "604cb85a37d74a9e915654caebc16f29",
            "ca65fae6252a43438d107087bbb14071",
            "ceb617345f3f44a8a4b130bef593b377",
            "3d8feada96cb4677b75d6e2ffc774ada",
            "f7714126768646249a4be23d713e60ab",
            "4c30ad7c31de47bba7be7f2ec8bea0bc",
            "7044fa4015e542378cea06aa3ce005f4",
            "d5b0690f8ac54154b53ed8d006b37056",
            "f6cd98a8341c4787be3f69fdb0edb14d",
            "3099d0e9859140128fa6260447660ac6",
            "4f1247340c964a16a8556438dd448208",
            "c192df25d1d242f9b13b4b26f8b8db56",
            "d1cd3c6b6b924afab0efabcd4a615c15",
            "af63a41560c24c97b071f466873ed415",
            "e97b83d49d1b4dd4ba29a82f56468b07",
            "8ed2703e22474f12b4a32b8e8f8ba1c4",
            "7d879bfd95a1404ba53f766ab943c407",
            "9b3b0a373c2a409099c4d93284fdec71",
            "e4580c6133a94fa9a6bc20095720cd8c",
            "1cd20373f519423c89423126be6e0c56",
            "3f40bdc9f8344081bada50d389244ce9",
            "87cadecc3a4b46b4ba79b3b950af22cc",
            "9ce89fe5cf764ab6bd86f2a31a1ddd96",
            "02b96cc0cd85436180152e0f0ab4c998",
            "a01e706b02414ec18951b2333c59ad73",
            "9b447dec163b41f29aaf1c9137e38e45",
            "67897a538935401aa658a271c70bbde0",
            "d722c01f56a64fc9b8301df511f8c818",
            "9422a5e6cfd84094843db3f58bd1cf6b",
            "97ceb6468d294f8f91f1f3c2216e122f",
            "570cb01b971c492e936aa3c33b6520c6",
            "c4ac6ef95761477b84f40e83c0d7f821",
            "a4510887c5a34ceeb45b52e5293baafc",
            "7ed31f011b0b4fed8bbff54ac5db7563",
            "fd7aed9a8b694f27b3411cdd61a7b8b7",
            "dd9a7b23b60e4f40a13b8f1123cf8906",
            "0b2bdde5763d4e0583847c629d4dc318",
            "0c2611338dea47d8b940ea5865fd8c27",
            "8b796046d8b446d8b7ab7991d6f60baf",
            "dada541b0742457aa7d3526acb5e3c46",
            "1be856435fc142f1afcc625b6ec8860d",
            "bd1727176dc14a7dbb4fc87b291eb686",
            "c189cdf0ebe9452d930a485698875f85",
            "01953e454a0249f9af439cb465cdb60c",
            "7bcf48d524174123832282ebeb7ccbc8",
            "6c822dc9189142b680b1be2f87f88b7f",
            "7cae5f4d813e4416ad6cae8afa542296",
            "cc6149938e89467cae17cedff27cb684",
            "14a1ca0b096a4699b9fa568880069dc1",
            "da136436fe904e99ababd5470c42d89e",
            "34c47d5435da4cc5bdc029a7af602d49",
            "07b74947de7f43e7ade7652e8283d820",
            "a832e2d83e1e487ea889d5ed073d2572",
            "2a9f3d0ade23485ea4158f82c2432d8f",
            "4abaf3b677984b0db57e23b6bacc6416",
            "f351587df885421182dd222d37da3121",
            "9571654bf66f496eb764ffbf0b24d38b",
            "e6d9a2316b934508b28f8c94f505ff04",
            "388eb8a409ae4111bb261dc5d5194457",
            "081517b37402402190126d220ab9fd30",
            "e5d808b976cf4f009f37730096f8e16a",
            "29718aaa09ef430ebee844fedc51a79b",
            "a57f46569194488aa95f66e9802a1dc5",
            "3f27b5e734c0400884f943bab676f7a3",
            "dddd41dbba7242e291491a616fe5433f",
            "d56cdde14cdc407ea44bce648d907b49",
            "6af14ce42bc443b39e96c234d405a00c",
            "cb4b5ffb2423484dbd4f973830f9dd9b",
            "3ba3861666394ef184a3ce83cc10ca46",
            "b403939762004e05a0362cb95a8f63f0",
            "eb3eba861cc94cd0967bd422e9587338",
            "626bc714fa824931beed9b13f970000d"
          ]
        },
        "id": "Yx2aICc-FAcC",
        "outputId": "b986ea63-ddfb-4f06-d324-d3ceb2f57382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Variant folder /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0 has no dataset_info.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db29ffedc89a4d74a58ea7e048bb81df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c518a52805a41db85d669d9a45199a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2da35449134c4b82b5fee72f04ebd0db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7044fa4015e542378cea06aa3ce005f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.2R0ROY_1.0.0/imdb_reviews-train.tfrecor\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b3b0a373c2a409099c4d93284fdec71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9422a5e6cfd84094843db3f58bd1cf6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.2R0ROY_1.0.0/imdb_reviews-test.tfrecord\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dada541b0742457aa7d3526acb5e3c46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34c47d5435da4cc5bdc029a7af602d49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.2R0ROY_1.0.0/imdb_reviews-unsupervised.\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29718aaa09ef430ebee844fedc51a79b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing Data"
      ],
      "metadata": {
        "id": "J5kLPIvqFETz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X_batch, y_batch):\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "    return X_batch, y_batch"
      ],
      "metadata": {
        "id": "4_cf3kRIFKdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]"
      ],
      "metadata": {
        "id": "aoKRnwoLICVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Engineering"
      ],
      "metadata": {
        "id": "58k4sEwxEf2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_id = len(tokenizer.word_index)\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "sUfsOhVdEbNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Buiding Model stateful GRU"
      ],
      "metadata": {
        "id": "vNTqz9UWErr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_char_rnn = keras.models.Sequential([\n",
        "    keras.layers.InputLayer(input_shape=[None, max_id], batch_size=batch_size),\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
        "                     dropout=0.2),\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
        "])\n",
        "model_char_rnn.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history_char_rnn = model_char_rnn.fit(dataset, epochs=10, callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwOCnYD_EnmX",
        "outputId": "9240c552-f833-4715-8201-4fe4750f1b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3920/3920\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 116ms/step - loss: 2.0810\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3920/3920\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 116ms/step - loss: 1.7004\n",
            "Epoch 3/10\n",
            "\u001b[1m3920/3920\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 117ms/step - loss: 1.6564\n",
            "Epoch 4/10\n",
            "\u001b[1m3920/3920\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m511s\u001b[0m 119ms/step - loss: 1.6380\n",
            "Epoch 5/10\n",
            "\u001b[1m3920/3920\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 116ms/step - loss: 1.6266\n",
            "Epoch 6/10\n",
            "\u001b[1m3920/3920\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m512s\u001b[0m 118ms/step - loss: 1.6195\n",
            "Epoch 7/10\n",
            "\u001b[1m3920/3920\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 119ms/step - loss: 1.6140\n",
            "Epoch 8/10\n",
            "\u001b[1m3920/3920\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 119ms/step - loss: 1.6103\n",
            "Epoch 9/10\n",
            "\u001b[1m3920/3920\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 117ms/step - loss: 1.6064\n",
            "Epoch 10/10\n",
            "\u001b[1m3920/3920\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m510s\u001b[0m 119ms/step - loss: 1.6038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, text, n_chars=50, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        X_new = np.array(tokenizer.texts_to_sequences([text])) - 1\n",
        "        X_one_hot = tf.one_hot(X_new, depth=max_id)\n",
        "        y_proba = model.predict(X_one_hot)[0, -1:, :]\n",
        "        rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "        char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "        text += tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
        "    return text"
      ],
      "metadata": {
        "id": "8OGs2nBqEu2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##pipeline"
      ],
      "metadata": {
        "id": "0c3z20u1FSRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "batch_size = 32\n",
        "\n",
        "text_vec_layer = keras.layers.TextVectorization(max_tokens=vocab_size, output_sequence_length=100)\n",
        "text_vec_layer.adapt(train_set.map(lambda x, y: x))\n",
        "\n",
        "# Preprocessing pipeline\n",
        "train_set_proc = train_set.batch(batch_size).map(preprocess)\n",
        "train_set_proc = train_set_proc.map(lambda X, y: (text_vec_layer(X), y)).prefetch(1)\n",
        "\n",
        "test_set_proc = test_set.batch(batch_size).map(preprocess)\n",
        "test_set_proc = test_set_proc.map(lambda X, y: (text_vec_layer(X), y)).prefetch(1)"
      ],
      "metadata": {
        "id": "lXaUfdMZFVax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building Model Sentiment Analysis"
      ],
      "metadata": {
        "id": "Qtll4uKsFj02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "model_sentiment = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model_sentiment.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history_sentiment = model_sentiment.fit(train_set_proc, epochs=20, validation_data=test_set_proc,callbacks=callbacks)"
      ],
      "metadata": {
        "id": "1LO4CZwLFZqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f22e9cae-7d57-4f79-f23d-fe96bd100031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 26ms/step - accuracy: 0.5981 - loss: 0.6427 - val_accuracy: 0.7453 - val_loss: 0.5022\n",
            "Epoch 2/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 23ms/step - accuracy: 0.7525 - loss: 0.4975 - val_accuracy: 0.7522 - val_loss: 0.4915\n",
            "Epoch 3/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.7719 - loss: 0.4647 - val_accuracy: 0.7542 - val_loss: 0.4893\n",
            "Epoch 4/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.7883 - loss: 0.4395 - val_accuracy: 0.7529 - val_loss: 0.4937\n",
            "Epoch 5/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.8023 - loss: 0.4143 - val_accuracy: 0.7476 - val_loss: 0.5082\n",
            "Epoch 6/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.8160 - loss: 0.3904 - val_accuracy: 0.7418 - val_loss: 0.5239\n",
            "Epoch 7/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - accuracy: 0.8349 - loss: 0.3655 - val_accuracy: 0.7364 - val_loss: 0.5550\n",
            "Epoch 8/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8521 - loss: 0.3359 - val_accuracy: 0.7323 - val_loss: 0.6013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Block Transformer"
      ],
      "metadata": {
        "id": "lNr4SWJhF20o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(keras.layers.Layer):\n",
        "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        if max_dims % 2 == 1: max_dims += 1\n",
        "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
        "        pos_emb = np.empty((1, max_steps, max_dims))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
        "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
        "    def call(self, inputs):\n",
        "        shape = tf.shape(inputs)\n",
        "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
      ],
      "metadata": {
        "id": "f4-8XMjnFr2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    def __init__(self, n_heads, d_model, causal=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model # Store d_model\n",
        "        self.causal = causal\n",
        "    def build(self,input_shapes):\n",
        "        self.d_keys = input_shapes[0][-1] # d_keys is the dimension of the input\n",
        "        self.d_values = input_shapes[0][-1] # d_values is the dimension of the input\n",
        "        # self.d_model is now passed in __init__\n",
        "        self.wq = [keras.layers.Dense(self.d_keys) for _ in range(self.n_heads)]\n",
        "        self.wk = [keras.layers.Dense(self.d_keys) for _ in range(self.n_heads)]\n",
        "        self.wv = [keras.layers.Dense(self.d_values) for _ in range(self.n_heads)]\n",
        "        self.wo = keras.layers.Dense(self.d_model) # Output dense layer with d_model units\n",
        "    def call(self, inputs):\n",
        "        q, v, k = inputs\n",
        "        heads = []\n",
        "        for i in range(self.n_heads):\n",
        "            q_head, k_head, v_head = self.wq[i](q), self.wk[i](k), self.wv[i](v)\n",
        "            k_transposed = tf.transpose(k_head, [0, 2, 1])\n",
        "            attention_scores = (q_head @ k_transposed) / tf.math.sqrt(tf.cast(self.d_keys, dtype=tf.float32)) # Cast to float32\n",
        "            attention_weights = tf.nn.softmax(attention_scores)\n",
        "            attention_head = attention_weights @ v_head\n",
        "            heads.append(attention_head)\n",
        "        heads_concatenated = tf.concat(heads, axis=-1)\n",
        "        return self.wo(heads_concatenated)"
      ],
      "metadata": {
        "id": "oEP6XkojF61W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "    def __init__(self, n_heads, d_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention = MultiHeadAttention(n_heads, d_model)\n",
        "        self.norm1 = keras.layers.LayerNormalization()\n",
        "        self.norm2 = keras.layers.LayerNormalization()\n",
        "        self.feed_forward = keras.models.Sequential([\n",
        "            keras.layers.Dense(d_model * 4, activation=\"relu\"),\n",
        "            keras.layers.Dense(d_model)\n",
        "        ])\n",
        "    def call(self, inputs):\n",
        "        attention_output = self.attention([inputs, inputs, inputs])\n",
        "        x = self.norm1(inputs + attention_output)\n",
        "        ff_output = self.feed_forward(x)\n",
        "        return self.norm2(x + ff_output)"
      ],
      "metadata": {
        "id": "7iJrGeEOF-C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "embed_size = 128\n",
        "n_heads = 8\n",
        "d_model = embed_size\n",
        "\n",
        "input_sequences = np.random.randint(vocab_size, size=(2, 10))"
      ],
      "metadata": {
        "id": "5-j_BtJfIuud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = keras.layers.Embedding(vocab_size, embed_size)\n",
        "pos_encoding_layer = PositionalEncoding(max_steps=10, max_dims=embed_size)\n",
        "transformer_block = TransformerBlock(n_heads=n_heads, d_model=d_model) # Pass d_model here"
      ],
      "metadata": {
        "id": "KuOMrudWI4zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_output = embedding_layer(input_sequences)\n",
        "pos_encoded_output = pos_encoding_layer(embedding_output)\n",
        "transformer_output = transformer_block(pos_encoded_output)"
      ],
      "metadata": {
        "id": "ylMfaOLSI2NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_encoded_output.shape)\n",
        "print(transformer_output.shape)\n",
        "print(transformer_output[0, 0, :10])"
      ],
      "metadata": {
        "id": "YOiiDR4pI64S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46159953-b37c-4386-8389-55568abb2d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 10, 128)\n",
            "(2, 10, 128)\n",
            "tf.Tensor(\n",
            "[-3.0446513   1.6366378  -0.3542529   0.41864032  0.10543233  0.26620412\n",
            "  0.9393467   0.46621236 -0.9112601   2.4177976 ], shape=(10,), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}
